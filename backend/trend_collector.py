import os
import asyncio
import aiohttp
from datetime import datetime
from typing import List, Dict, Any
import json
from dotenv import load_dotenv
from bs4 import BeautifulSoup
import feedparser

load_dotenv()

class ImprovedTrendCollector:
    """ê°œì„ ëœ íŠ¸ë Œë“œ ìˆ˜ì§‘ê¸° - ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ì— ìµœì í™”"""
    
    def __init__(self):
        self.session = None
        # API í‚¤ ë¡œë“œ
        self.youtube_api_key = os.getenv('YOUTUBE_API_KEY')
        self.news_api_key = os.getenv('NEWS_API_KEY')
        self.naver_client_id = os.getenv('NAVER_CLIENT_ID')
        self.naver_client_secret = os.getenv('NAVER_CLIENT_SECRET')
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.session.close()
    
    async def collect_all(self) -> Dict[str, Any]:
        """ëª¨ë“  ì†ŒìŠ¤ì—ì„œ íŠ¸ë Œë“œ ìˆ˜ì§‘"""
        tasks = [
            self.collect_youtube_api(),
            self.collect_naver_api(),
            self.collect_news_api(),
            self.collect_google_rss(),
            self.collect_daum_search(),
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ì„±ê³µí•œ ê²°ê³¼ë§Œ í•„í„°ë§
        all_trends = []
        for i, result in enumerate(results):
            if isinstance(result, list):
                all_trends.extend(result)
            else:
                print(f"ìˆ˜ì§‘ ì‹¤íŒ¨ [{i}]: {result}")
        
        return {
            "timestamp": datetime.now().isoformat(),
            "total": len(all_trends),
            "trends": all_trends
        }
    
    async def collect_youtube_api(self) -> List[Dict[str, Any]]:
        """YouTube APIë¡œ ì‹¤ì œ ì¸ê¸° ë™ì˜ìƒ ìˆ˜ì§‘"""
        if not self.youtube_api_key or self.youtube_api_key == 'your_youtube_api_key_here':
            print("YouTube API í‚¤ ë¯¸ì„¤ì •")
            return []
        
        try:
            from .collectors.youtube_collector import YouTubeCollector
            collector = YouTubeCollector()
            videos = collector.fetch_trending_videos(max_results=50)  # 50ê°œë¡œ ì¦ê°€
            
            trends = []
            for video in videos:
                trends.append({
                    "keyword": video['keyword'],
                    "source": "youtube",
                    "score": video.get('view_count', 0) // 10000,  # ë§Œ ë‹¨ìœ„ë¡œ ì ìˆ˜í™”
                    "url": video.get('url', ''),
                    "metadata": {
                        "channel": video.get('channel', ''),
                        "views": video.get('view_count', 0),
                        "description": video.get('description', '')[:200],
                        "published_at": video.get('published_at', ''),
                        "thumbnail": video.get('thumbnail', '')
                    },
                    "timestamp": datetime.now().isoformat()
                })
            
            print(f"YouTube: {len(trends)}ê°œ ìˆ˜ì§‘")
            return trends
            
        except Exception as e:
            print(f"YouTube API ì˜¤ë¥˜: {e}")
            return []
    
    async def collect_naver_api(self) -> List[Dict[str, Any]]:
        """ë„¤ì´ë²„ APIë¡œ ì‹¤ì‹œê°„ ë‰´ìŠ¤/ê²€ìƒ‰ì–´ ìˆ˜ì§‘"""
        if not self.naver_client_id:
            print("Naver API í‚¤ ë¯¸ì„¤ì •")
            return []
        
        try:
            trends = []
            
            # 1. ì‹¤ì‹œê°„ ê¸‰ìƒìŠ¹ ë‰´ìŠ¤
            url = "https://openapi.naver.com/v1/search/news.json"
            headers = {
                "X-Naver-Client-Id": self.naver_client_id,
                "X-Naver-Client-Secret": self.naver_client_secret,
            }
            
            # ì—¬ëŸ¬ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•˜ì—¬ íŠ¸ë Œë“œ íŒŒì•…
            hot_keywords = ["ì†ë³´", "ë‹¨ë…", "ê¸´ê¸‰", "í™”ì œ", "ë…¼ë€", "ì¸ê¸°"]
            
            for keyword in hot_keywords:
                params = {
                    "query": keyword,
                    "display": 5,
                    "sort": "date"
                }
                
                async with self.session.get(url, headers=headers, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        for item in data.get('items', []):
                            # HTML íƒœê·¸ ì œê±°
                            import re
                            title = re.sub('<.*?>', '', item['title'])
                            
                            # í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ì‹)
                            words = title.split()[:5]  # ì• 5ë‹¨ì–´
                            main_keyword = ' '.join(words)
                            
                            trends.append({
                                "keyword": main_keyword,
                                "source": "naver",
                                "score": 50 + (hot_keywords.index(keyword) * 10),
                                "url": item.get('link', ''),
                                "timestamp": datetime.now().isoformat()
                            })
            
            # ì¤‘ë³µ ì œê±°
            seen = set()
            unique_trends = []
            for trend in trends:
                if trend['keyword'] not in seen:
                    seen.add(trend['keyword'])
                    unique_trends.append(trend)
            
            print(f"Naver: {len(unique_trends)}ê°œ ìˆ˜ì§‘")
            return unique_trends[:50]  # 50ê°œë¡œ ì¦ê°€
            
        except Exception as e:
            print(f"Naver API ì˜¤ë¥˜: {e}")
            return []
    
    async def collect_news_api(self) -> List[Dict[str, Any]]:
        """ì—°í•©ë‰´ìŠ¤ RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        try:
            # ì—°í•©ë‰´ìŠ¤ RSS í”¼ë“œ URL
            rss_url = 'https://www.yna.co.kr/rss/news.xml'
            
            async with self.session.get(rss_url) as response:
                if response.status == 200:
                    import xml.etree.ElementTree as ET
                    
                    text = await response.text()
                    root = ET.fromstring(text)
                    
                    trends = []
                    items = root.findall('.//item')[:50]  # ìµœì‹  ë‰´ìŠ¤ 50ê°œ
                    
                    for idx, item in enumerate(items):
                        title_elem = item.find('title')
                        desc_elem = item.find('description')
                        link_elem = item.find('link')
                        
                        if title_elem is not None and title_elem.text:
                            title = title_elem.text
                            
                            # ì œëª©ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
                            # ì—°í•©ë‰´ìŠ¤ ì œëª© í˜•ì‹: "ì£¼ìš”ë‚´ìš©" ë˜ëŠ” "ì£¼ìš”ë‚´ìš©(ë¶€ê°€ì •ë³´)"
                            keyword = title.split('(')[0].strip()
                            if len(keyword) > 30:
                                keyword = keyword[:30] + '...'
                            
                            trends.append({
                                "keyword": keyword,
                                "source": "news",
                                "score": 80 - (idx * 2),  # ìµœì‹  ë‰´ìŠ¤ì¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
                                "url": link_elem.text if link_elem is not None else '',
                                "description": desc_elem.text[:200] if desc_elem is not None and desc_elem.text else '',
                                "timestamp": datetime.now().isoformat()
                            })
                    
                    print(f"ì—°í•©ë‰´ìŠ¤ì—ì„œ {len(trends)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
                    return trends
                else:
                    print(f"ì—°í•©ë‰´ìŠ¤ RSS ì ‘ê·¼ ì˜¤ë¥˜: {response.status}")
                    return []
                    
        except Exception as e:
            print(f"ì—°í•©ë‰´ìŠ¤ RSS ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return []
    
    async def collect_google_rss(self) -> List[Dict[str, Any]]:
        """Google Trends RSS í”¼ë“œ ìˆ˜ì§‘"""
        try:
            url = "https://trends.google.com/trends/trendingsearches/daily/rss?geo=KR"
            
            async with self.session.get(url) as response:
                if response.status == 200:
                    content = await response.text()
                    feed = feedparser.parse(content)
                    
                    trends = []
                    for idx, entry in enumerate(feed.entries[:10]):
                        trends.append({
                            "keyword": entry.title,
                            "source": "google",
                            "score": 100 - (idx * 5),  # ìˆœìœ„ì— ë”°ë¥¸ ì ìˆ˜
                            "timestamp": datetime.now().isoformat()
                        })
                    
                    print(f"Google RSS: {len(trends)}ê°œ ìˆ˜ì§‘")
                    return trends
                    
        except Exception as e:
            print(f"Google RSS ì˜¤ë¥˜: {e}")
            return []
    
    async def collect_daum_search(self) -> List[Dict[str, Any]]:
        """ë‹¤ìŒ ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ (ëŒ€ì²´ ë°©ë²•)"""
        try:
            # ë‹¤ìŒ ë‰´ìŠ¤ ì¸ê¸° ê¸°ì‚¬ë¡œ ëŒ€ì²´
            url = "https://news.daum.net/ranking/popular"
            
            async with self.session.get(url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    trends = []
                    # ì¸ê¸° ë‰´ìŠ¤ ì œëª© ì¶”ì¶œ
                    news_items = soup.select('a.link_txt')[:10]
                    
                    for idx, item in enumerate(news_items):
                        title = item.text.strip()
                        if title:
                            # ì œëª©ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
                            words = title.split()[:5]
                            keyword = ' '.join(words)
                            
                            trends.append({
                                "keyword": keyword,
                                "source": "daum",
                                "score": 80 - (idx * 5),
                                "timestamp": datetime.now().isoformat()
                            })
                    
                    print(f"Daum: {len(trends)}ê°œ ìˆ˜ì§‘")
                    return trends
                    
        except Exception as e:
            print(f"Daum ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return []

def analyze_trends(all_trends: List[Dict[str, Any]]) -> Dict[str, Any]:
    """ìˆ˜ì§‘ëœ íŠ¸ë Œë“œ ë¶„ì„ ë° ì •ë¦¬"""
    # í‚¤ì›Œë“œë³„ ì ìˆ˜ ì§‘ê³„
    keyword_scores = {}
    
    for trend in all_trends:
        keyword = trend['keyword'].lower()
        if keyword in keyword_scores:
            keyword_scores[keyword]['score'] += trend.get('score', 50)
            keyword_scores[keyword]['sources'].add(trend['source'])
            keyword_scores[keyword]['count'] += 1
        else:
            keyword_scores[keyword] = {
                'keyword': trend['keyword'],  # ì›ë³¸ ìœ ì§€
                'score': trend.get('score', 50),
                'sources': {trend['source']},
                'count': 1,
                'urls': []
            }
        
        # URL ìˆ˜ì§‘
        if trend.get('url'):
            keyword_scores[keyword]['urls'].append(trend['url'])
    
    # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
    sorted_keywords = sorted(
        keyword_scores.values(),
        key=lambda x: x['score'] * len(x['sources']),  # ì ìˆ˜ x í”Œë«í¼ ìˆ˜
        reverse=True
    )
    
    # ìƒìœ„ 100ê°œë§Œ ì„ íƒ
    top_keywords = sorted_keywords[:100]
    
    # API í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    formatted_trends = []
    for idx, item in enumerate(top_keywords):
        formatted_trends.append({
            'keyword': item['keyword'],
            'score': item['score'],
            'sources': list(item['sources']),
            'rank': idx + 1,
            'timestamp': datetime.now().isoformat()
        })
    
    return {
        'hot_keywords': formatted_trends,
        'total_collected': len(all_trends),
        'unique_keywords': len(keyword_scores),
        'last_update': datetime.now().isoformat()
    }

async def test_collection():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("=" * 50)
    print("ê°œì„ ëœ íŠ¸ë Œë“œ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    async with ImprovedTrendCollector() as collector:
        # ì „ì²´ ìˆ˜ì§‘
        result = await collector.collect_all()
        
        print(f"\nì´ {result['total']}ê°œ íŠ¸ë Œë“œ ìˆ˜ì§‘")
        
        # ë¶„ì„
        analysis = analyze_trends(result['trends'])
        
        print(f"\nğŸ”¥ HOT í‚¤ì›Œë“œ TOP 10:")
        for trend in analysis['hot_keywords'][:10]:
            sources = ', '.join(trend['sources'])
            print(f"{trend['rank']}. {trend['keyword']} (ì ìˆ˜: {trend['score']}, ì¶œì²˜: {sources})")
        
        # ê²°ê³¼ ì €ì¥
        os.makedirs('results', exist_ok=True)
        with open('results/test_trends.json', 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… ê²°ê³¼ ì €ì¥: results/test_trends.json")

if __name__ == "__main__":
    asyncio.run(test_collection())